{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "comment_types = [\"no_comments\", \"comments\", \"added_test_comments\", \"added_code_comments\", \"added_CT_comments\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment_type in comment_types:\n",
    "    for val in [\"all\", \"50\", \"25\", \"10\", \"05\"]:\n",
    "        df = pd.read_csv(f\"./fold0/{comment_type}/project_stats_{val}.csv\")\n",
    "        # Simplify project names\n",
    "        df[\"project\"] = df.apply(lambda row: re.split(r\"-\\d\", row[\"project\"])[0], axis=1)\n",
    "        df.rename(\n",
    "            columns={\n",
    "                \"accuracy\": \"Accuracy\",\n",
    "                \"pass_accuracy\": \"Pass Class Accuracy\",\n",
    "                \"fail_accuracy\": \"Fail Class Accuracy\",\n",
    "                \"pass_rate\": \"Dataset Pass Rate\",\n",
    "                \"fail_rate\": \"Dataset Fail Rate\",\n",
    "                \"accuracy_improvement\": \"Accuracy \\Delta\",\n",
    "                \"fail_accuracy_improvement\": \"Fail Accuracy \\Delta\",\n",
    "                \"f1\": \"F1\",\n",
    "                \"f1_improvement\": \"F1 \\Delta\",\n",
    "                \"coin_accuracy\": \"Coin Accuracy\",\n",
    "                \"out_vocab_C_ratio\": \"Missing MUT Token Rate\",\n",
    "                \"out_vocab_T_ratio\": \"Missing Test Token Rate\",\n",
    "                \"out_vocab_combined_ratio\": \"Missing Overall Token Rate\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        table1 = df[\n",
    "            [\n",
    "                \"project\",\n",
    "                \"N\",\n",
    "                \"Dataset Pass Rate\",\n",
    "                \"Dataset Fail Rate\",\n",
    "                \"Missing MUT Token Rate\",\n",
    "                \"Missing Test Token Rate\",\n",
    "                \"Missing Overall Token Rate\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        table2 = df[\n",
    "            [\n",
    "                \"project\",\n",
    "                \"Fail Accuracy \\Delta\",\n",
    "                \"Accuracy \\Delta\",\n",
    "                \"F1 \\Delta\",\n",
    "                \"Accuracy\",\n",
    "                \"Pass Class Accuracy\",\n",
    "                \"Fail Class Accuracy\",\n",
    "                \"F1\",\n",
    "                \"Coin Accuracy\",\n",
    "                \"tp\",\n",
    "                \"fn\",\n",
    "                \"tn\",\n",
    "                \"fp\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        if val == \"all\":\n",
    "            table1.to_latex(\n",
    "                f\"./latex/{comment_type}/dataset_stats_{val}.tex\",\n",
    "                index=False,\n",
    "                caption=f\"New Dataset Statistics ({comment_type})\",\n",
    "                label=f\"tab:stats_{val}\",\n",
    "            )\n",
    "            table2.to_latex(\n",
    "                f\"./latex/{comment_type}/results_{val}.tex\",\n",
    "                index=False,\n",
    "                caption=f\"SEER Results on New Data ({comment_type}), sorted by failure accuracy $\\Delta$\",\n",
    "                label=f\"tab:results_{val}\",\n",
    "            )\n",
    "        else:\n",
    "            table2 = df[\n",
    "                [\n",
    "                    \"project\",\n",
    "                    \"N\",\n",
    "                    \"Fail Accuracy \\Delta\",\n",
    "                    \"Accuracy \\Delta\",\n",
    "                    \"F1 \\Delta\",\n",
    "                    \"Accuracy\",\n",
    "                    \"Pass Class Accuracy\",\n",
    "                    \"Fail Class Accuracy\",\n",
    "                    \"F1\",\n",
    "                    \"Coin Accuracy\",\n",
    "                    \"tp\",\n",
    "                    \"fn\",\n",
    "                    \"tn\",\n",
    "                    \"fp\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            # table1.to_latex(\n",
    "            #     f\"./latex/{comment_type}/dataset_stats_{val}.tex\",\n",
    "            #     index=False,\n",
    "            #     caption=f\"New Dataset Statistics ({comment_type}), restricted to minimum {val}\\% of tokens present\",\n",
    "            #     label=f\"tab:stats_{val}\",\n",
    "            # )\n",
    "            table2.to_latex(\n",
    "                f\"./latex/{comment_type}/results_{val}.tex\",\n",
    "                index=False,\n",
    "                caption=f\"SEER Results on New Data ({comment_type}), restricted to minimum {str(100-int(val))}\\% of tokens present\",\n",
    "                label=f\"tab:results_{val}\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing some LaTeX issues\n",
    "for comment_type in comment_types:\n",
    "\n",
    "    for filename in os.listdir(f\"./latex/{comment_type}\"):\n",
    "        with open(f\"./latex/{comment_type}/{filename}\", \"r+\") as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r\"\\\\textbackslash Delta\", \"$\\Delta$\", text)\n",
    "            text = re.sub(r\"\\\\textbackslash delta\", \"$\\delta$\", text)\n",
    "            text = re.sub(\"table\", \"table*\", text)\n",
    "            text = re.sub(\"_comments\", \" comments\", text)\n",
    "            f.seek(0)\n",
    "            f.write(text)\n",
    "            f.truncate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_type = \"no_comments\"\n",
    "thresholds = [\"all\", \"50\", \"25\", \"20\", \"15\", \"10\"]\n",
    "for val in thresholds:\n",
    "    df = pd.read_csv(f\"./fold0/{comment_type}/project_stats_{val}.csv\")\n",
    "    # Simplify project names\n",
    "    df[\"project\"] = df.apply(lambda row: re.split(r\"-\\d\", row[\"project\"])[0], axis=1)\n",
    "    table2 = df[[\"project\", \"N\", \"fail_accuracy_improvement\", \"accuracy_improvement\", \"f1_improvement\"]]\n",
    "\n",
    "    if val == \"all\":\n",
    "        df_merge = table2.copy()\n",
    "    else:\n",
    "        df_merge = df_merge.merge(table2, on=\"project\", how=\"left\")\n",
    "        # print(df_merge.columns)\n",
    "        df_merge.rename(\n",
    "            columns={\n",
    "                \"N_x\": f\"N_{last}\",\n",
    "                \"N_y\": f\"N_{val}\",\n",
    "                \"fail_accuracy_improvement_x\": f\"fail_accuracy_improvement_{last}\",\n",
    "                \"fail_accuracy_improvement_y\": f\"fail_accuracy_improvement_{val}\",\n",
    "                \"accuracy_improvement_x\": f\"accuracy_improvement_{last}\",\n",
    "                \"accuracy_improvement_y\": f\"accuracy_improvement_{val}\",\n",
    "                \"f1_improvement_x\": f\"f1_improvement_{last}\",\n",
    "                \"f1_improvement_y\": f\"f1_improvement_{val}\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "    last = val\n",
    "\n",
    "df_merge.to_csv(\"vocab_analysis.csv\")\n",
    "# df_merge[df_merge['project']=='all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_only_df = df_merge[df_merge[\"project\"] != \"all\"]\n",
    "min_sample = 20\n",
    "\n",
    "table_vocab_analysis = pd.DataFrame(\n",
    "    {\n",
    "        \"thresholds\": [\"50%\", \"25%\", \"20%\", \"15%\"],\n",
    "        \"N\": [df_merge.loc[25, \"N_50\"], df_merge.loc[25, \"N_25\"], df_merge.loc[25, \"N_20\"], df_merge.loc[25, \"N_15\"]],\n",
    "        \"fail_accuracy_improvement_total\": [\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_50\"],\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_25\"],\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_20\"],\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_15\"],\n",
    "        ],\n",
    "        \"accuracy_improvement_total\": [\n",
    "            df_merge.loc[25, \"accuracy_improvement_50\"],\n",
    "            df_merge.loc[25, \"accuracy_improvement_25\"],\n",
    "            df_merge.loc[25, \"accuracy_improvement_20\"],\n",
    "            df_merge.loc[25, \"accuracy_improvement_15\"],\n",
    "        ],\n",
    "        \"f1_improvement_total\": [\n",
    "            df_merge.loc[25, \"f1_improvement_50\"],\n",
    "            df_merge.loc[25, \"f1_improvement_25\"],\n",
    "            df_merge.loc[25, \"f1_improvement_20\"],\n",
    "            df_merge.loc[25, \"f1_improvement_15\"],\n",
    "        ],\n",
    "        \"fail_accuracy_improvement_avg\": [\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_50\"] > min_sample].index, \"fail_accuracy_improvement_50\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_25\"] > min_sample].index, \"fail_accuracy_improvement_25\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_20\"] > min_sample].index, \"fail_accuracy_improvement_20\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_15\"] > min_sample].index, \"fail_accuracy_improvement_15\"].mean(),\n",
    "        ],\n",
    "        \"accuracy_improvement_avg\": [\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_50\"] > min_sample].index, \"accuracy_improvement_50\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_25\"] > min_sample].index, \"accuracy_improvement_25\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_20\"] > min_sample].index, \"accuracy_improvement_20\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_15\"] > min_sample].index, \"accuracy_improvement_15\"].mean(),\n",
    "        ],\n",
    "        \"f1_improvement_avg\": [\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_50\"] > min_sample].index, \"f1_improvement_50\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_25\"] > min_sample].index, \"f1_improvement_25\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_20\"] > min_sample].index, \"f1_improvement_20\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_15\"] > min_sample].index, \"f1_improvement_15\"].mean(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "table_vocab_analysis = table_vocab_analysis.astype({\"N\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Maximum % Missing Vocabulary</th>\n",
       "      <th>N</th>\n",
       "      <th>Fail Accuracy \\delta</th>\n",
       "      <th>Accuracy \\delta</th>\n",
       "      <th>F1 \\delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50%</td>\n",
       "      <td>145474</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25%</td>\n",
       "      <td>35918</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20%</td>\n",
       "      <td>10156</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15%</td>\n",
       "      <td>2643</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Maximum % Missing Vocabulary       N  Fail Accuracy \\delta  Accuracy \\delta  \\\n",
       "0                          50%  145474                -0.000           -0.010   \n",
       "1                          25%   35918                -0.016            0.011   \n",
       "2                          20%   10156                -0.050           -0.007   \n",
       "3                          15%    2643                -0.022           -0.074   \n",
       "\n",
       "   F1 \\delta  \n",
       "0     -0.005  \n",
       "1      0.006  \n",
       "2     -0.002  \n",
       "3     -0.045  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in [\"fail_accuracy_improvement_total\", \"accuracy_improvement_total\", \"f1_improvement_total\"]:\n",
    "    string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    table_vocab_analysis[col] = table_vocab_analysis[col].apply(lambda x: np.round(x - df_merge.loc[25, f\"{string}_all\"], 3))\n",
    "\n",
    "for col in [\"fail_accuracy_improvement_avg\", \"accuracy_improvement_avg\", \"f1_improvement_avg\"]:\n",
    "    string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    table_vocab_analysis[col] = table_vocab_analysis[col].apply(lambda x: np.round(x - project_only_df[f\"{string}_all\"].mean(), 3))\n",
    "\n",
    "table_vocab_analysis.drop(columns=[\"fail_accuracy_improvement_avg\", \"accuracy_improvement_avg\", \"f1_improvement_avg\"], inplace=True)\n",
    "\n",
    "table_vocab_analysis.rename(columns={\"thresholds\": \"Max % Missing Vocabulary\", \n",
    "                            \"fail_accuracy_improvement_total\": \"Fail Accuracy \\delta\", \n",
    "                            \"accuracy_improvement_total\": \"Accuracy \\delta\",\n",
    "                            \"f1_improvement_total\": \"F1 \\delta\",\n",
    "                            }, inplace=True)\n",
    "\n",
    "table_vocab_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_vocab_analysis.to_latex(\n",
    "    f\"./latex/vocab_analysis.tex\",\n",
    "    index=False,\n",
    "    caption=f\"Performance of SEER on New Data with varying minimum \\% of tokens in-vocab threshold.\",\n",
    "    label=f\"tab:vocab_analysis\",\n",
    ")\n",
    "\n",
    "with open(f\"./latex/vocab_analysis.tex\", \"r+\") as f:\n",
    "    text = f.read()\n",
    "    text = re.sub(r\"\\\\textbackslash delta\", \"$\\delta$\", text)\n",
    "    # text = re.sub(\"accuracy\", \"Accuracy\", text)\n",
    "    # text = re.sub(\"_improvement\", \"_$\\Delta$\", text)\n",
    "    # text = re.sub(\"_total\", \"_all\", text)\n",
    "    # text = re.sub(\"_avg\", \"_project\\_avg\", text)\n",
    "    f.seek(0)\n",
    "    f.write(text)\n",
    "    f.truncate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robustness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment_type in comment_types:\n",
    "    df = pd.read_csv(f\"./fold0/{comment_type}/project_stats_all.csv\")\n",
    "    # Simplify project names\n",
    "    df[\"project\"] = df.apply(lambda row: re.split(r\"-\\d\", row[\"project\"])[0], axis=1)\n",
    "    table2 = df[[\"project\", \"N\", \"fail_accuracy_improvement\", \"accuracy_improvement\", \"f1_improvement\"]].copy()\n",
    "    table2.rename(\n",
    "        columns={\n",
    "            \"N\": f\"N_{comment_type}\",\n",
    "            \"fail_accuracy_improvement\": f\"fail_accuracy_improvement_{comment_type}\",\n",
    "            \"accuracy_improvement\": f\"accuracy_improvement_{comment_type}\",\n",
    "            \"f1_improvement\": f\"f1_improvement_{comment_type}\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    if comment_type == comment_types[0]:\n",
    "        df_merge_comments = table2.copy()\n",
    "    else:\n",
    "        df_merge_comments = df_merge_comments.merge(table2, on=\"project\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_only_comments_df = df_merge_comments[df_merge_comments[\"project\"] != \"all\"]\n",
    "min_sample = 20\n",
    "\n",
    "table_comment_analysis = pd.DataFrame(\n",
    "    {\n",
    "        \"comment_types\": [\"No Comments\", \"Preserved Comments\", \"Added Test Comment\", \"Added MUT Comment\", \"Added MUT/Test Comments\"],\n",
    "        \"N\": [df_merge_comments.loc[25, f\"N_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"fail_accuracy_improvement_total\": [df_merge_comments.loc[25, f\"fail_accuracy_improvement_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"accuracy_improvement_total\": [df_merge_comments.loc[25, f\"accuracy_improvement_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"f1_improvement_total\": [df_merge_comments.loc[25, f\"f1_improvement_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"fail_accuracy_improvement_avg\": [\n",
    "            df_merge_comments.loc[project_only_comments_df[project_only_comments_df[f\"N_{comment_type}\"] > min_sample].index, f\"fail_accuracy_improvement_{comment_type}\"].mean()\n",
    "            for comment_type in comment_types\n",
    "        ],\n",
    "        \"accuracy_improvement_avg\": [\n",
    "            df_merge_comments.loc[project_only_comments_df[project_only_comments_df[f\"N_{comment_type}\"] > min_sample].index, f\"accuracy_improvement_{comment_type}\"].mean()\n",
    "            for comment_type in comment_types\n",
    "        ],\n",
    "        \"f1_improvement_avg\": [\n",
    "            df_merge_comments.loc[project_only_comments_df[project_only_comments_df[f\"N_{comment_type}\"] > min_sample].index, f\"f1_improvement_{comment_type}\"].mean()\n",
    "            for comment_type in comment_types\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "table_comment_analysis = table_comment_analysis.astype({\"N\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment Type</th>\n",
       "      <th>Fail Accuracy \\delta</th>\n",
       "      <th>Accuracy \\delta</th>\n",
       "      <th>F1 \\delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Comments</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preserved Comments</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Added Test Comment</td>\n",
       "      <td>-0.0086</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Added MUT Comment</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Added MUT/Test Comments</td>\n",
       "      <td>-0.0558</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Comment Type  Fail Accuracy \\delta  Accuracy \\delta  F1 \\delta\n",
       "0              No Comments                0.0000           0.0000     0.0000\n",
       "1       Preserved Comments                0.0055          -0.0002    -0.0002\n",
       "2       Added Test Comment               -0.0086           0.0005     0.0003\n",
       "3        Added MUT Comment               -0.0446           0.0096     0.0055\n",
       "4  Added MUT/Test Comments               -0.0558           0.0135     0.0077"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in [\"fail_accuracy_improvement_total\", \"accuracy_improvement_total\", \"f1_improvement_total\"]:\n",
    "    string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    table_comment_analysis[col] = table_comment_analysis[col].apply(lambda x: np.round(x - df_merge_comments.loc[25, f\"{string}_no_comments\"], 4))\n",
    "\n",
    "for col in [\"fail_accuracy_improvement_avg\", \"accuracy_improvement_avg\", \"f1_improvement_avg\"]:\n",
    "    table_comment_analysis.drop(columns=[col], inplace=True)\n",
    "    # string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    # table_comment_analysis[col] = table_comment_analysis[col].apply(lambda x: np.round(x - project_only_comments_df[f\"{string}_no_comments\"].mean(), 4))\n",
    "\n",
    "table_comment_analysis.drop(columns=[\"N\"], inplace=True)\n",
    "\n",
    "table_comment_analysis.rename(columns={\"comment_types\": \"Comment Type\",\n",
    "                            \"fail_accuracy_improvement_total\": \"Fail Accuracy \\delta\",\n",
    "                            \"accuracy_improvement_total\": \"Accuracy \\delta\",\n",
    "                            \"f1_improvement_total\": \"F1 \\delta\",\n",
    "                            }, inplace=True)\n",
    "table_comment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_comment_analysis.to_latex(\n",
    "    f\"./latex/comment_analysis.tex\",\n",
    "    index=False,\n",
    "    caption=f\"Performance of SEER on New Data with different comment types (compared to a no-comment baseline).\",\n",
    "    label=f\"tab:comment_analysis\",\n",
    ")\n",
    "\n",
    "with open(f\"./latex/comment_analysis.tex\", \"r+\") as f:\n",
    "    text = f.read()\n",
    "    # text = re.sub(\"table\", \"table*\", text)\n",
    "    text = re.sub(r\"\\\\textbackslash delta\", \"$\\delta$\", text)\n",
    "    text = re.sub(\"accuracy\", \"Accuracy\", text)\n",
    "    text = re.sub(\"\\\\\\_improvement\", \"\", text)\n",
    "    text = re.sub(\"_total\", \"_all\", text)\n",
    "    text = re.sub(\"_avg\", \"_project\\_avg\", text)\n",
    "    f.seek(0)\n",
    "    f.write(text)\n",
    "    f.truncate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_unique = pd.read_csv(f\"./similarity_analysis/similarity_unique_mut.csv\")\n",
    "# Simplify project names\n",
    "df_common_unique[\"triplets\"] = df_common_unique.apply(lambda row: re.split(r\"-\\d\", row[\"triplets\"])[0], axis=1)\n",
    "\n",
    "df_common_unique.rename(\n",
    "    columns={\n",
    "        \"phase2\": \"SEER\",\n",
    "        \"triplets\": \"New Data\",\n",
    "        \"triplets_unique_count\": \"New Data Count\",\n",
    "        \"phase2_unique_count\": \"SEER Count\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df_common_unique[[\"SEER\", \"New Data\", \"SEER Count\", \"New Data Count\"]].to_latex(\n",
    "    f\"./latex/common_projects_unique.tex\",\n",
    "    index=False,\n",
    "    caption=f\"Unique Methods Under Test\",\n",
    "    label=f\"tab:common_unique_MUT\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
