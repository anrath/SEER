{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "comment_types = [\"no_comments\", \"comments\", \"added_test_comments\", \"added_code_comments\", \"added_CT_comments\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment_type in comment_types:\n",
    "    for val in [\"all\", \"50\", \"25\", \"10\", \"05\"]:\n",
    "        df = pd.read_csv(f\"./fold0/{comment_type}/project_stats_{val}.csv\")\n",
    "        # Simplify project names\n",
    "        df[\"project\"] = df.apply(lambda row: re.split(r\"-\\d\", row[\"project\"])[0], axis=1)\n",
    "        df.rename(\n",
    "            columns={\n",
    "                \"accuracy\": \"acc.\",\n",
    "                \"pass_accuracy\": \"pass_acc.\",\n",
    "                \"fail_accuracy\": \"fail_acc.\",\n",
    "                \"pass_rate\": \"dataset_pass_%\",\n",
    "                \"fail_rate\": \"dataset_fail_%\",\n",
    "                \"accuracy_improvement\": \"acc_\\Delta\",\n",
    "                \"fail_accuracy_improvement\": \"fail_acc_\\Delta\",\n",
    "                \"f1_improvement\": \"f1_\\Delta\",\n",
    "                \"coin_accuracy\": \"coin_acc.\",\n",
    "                \"out_vocab_C_ratio\": \"missing_C_%\",\n",
    "                \"out_vocab_T_ratio\": \"missing_T_%\",\n",
    "                \"out_vocab_combined_ratio\": \"missing_token_%\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        table1 = df[\n",
    "            [\n",
    "                \"project\",\n",
    "                \"N\",\n",
    "                \"dataset_pass_%\",\n",
    "                \"dataset_fail_%\",\n",
    "                \"missing_C_%\",\n",
    "                \"missing_T_%\",\n",
    "                \"missing_token_%\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        table2 = df[\n",
    "            [\n",
    "                \"project\",\n",
    "                \"fail_acc_\\Delta\",\n",
    "                \"acc_\\Delta\",\n",
    "                \"f1_\\Delta\",\n",
    "                \"acc.\",\n",
    "                \"pass_acc.\",\n",
    "                \"fail_acc.\",\n",
    "                \"f1\",\n",
    "                \"coin_acc.\",\n",
    "                \"coin_f1\",\n",
    "                \"tp\",\n",
    "                \"fn\",\n",
    "                \"tn\",\n",
    "                \"fp\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        if val == \"all\":\n",
    "            table1.to_latex(\n",
    "                f\"./latex/{comment_type}/dataset_stats_{val}.tex\",\n",
    "                index=False,\n",
    "                caption=f\"New Dataset Statistics ({comment_type})\",\n",
    "                label=f\"tab:stats_{val}\",\n",
    "            )\n",
    "            table2.to_latex(\n",
    "                f\"./latex/{comment_type}/results_{val}.tex\",\n",
    "                index=False,\n",
    "                caption=f\"SEER Results on New Data ({comment_type}), sorted by failure accuracy $\\Delta$\",\n",
    "                label=f\"tab:results_{val}\",\n",
    "            )\n",
    "        else:\n",
    "            table2 = df[\n",
    "                [\n",
    "                    \"project\",\n",
    "                    \"N\",\n",
    "                    \"fail_acc_\\Delta\",\n",
    "                    \"acc_\\Delta\",\n",
    "                    \"f1_\\Delta\",\n",
    "                    \"acc.\",\n",
    "                    \"pass_acc.\",\n",
    "                    \"fail_acc.\",\n",
    "                    \"f1\",\n",
    "                    \"coin_acc.\",\n",
    "                    \"coin_f1\",\n",
    "                    \"tp\",\n",
    "                    \"fn\",\n",
    "                    \"tn\",\n",
    "                    \"fp\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            # table1.to_latex(\n",
    "            #     f\"./latex/{comment_type}/dataset_stats_{val}.tex\",\n",
    "            #     index=False,\n",
    "            #     caption=f\"New Dataset Statistics ({comment_type}), restricted to minimum {val}\\% of tokens present\",\n",
    "            #     label=f\"tab:stats_{val}\",\n",
    "            # )\n",
    "            table2.to_latex(\n",
    "                f\"./latex/{comment_type}/results_{val}.tex\",\n",
    "                index=False,\n",
    "                caption=f\"SEER Results on New Data ({comment_type}), restricted to minimum {str(100-int(val))}\\% of tokens present\",\n",
    "                label=f\"tab:results_{val}\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing some LaTeX issues\n",
    "for comment_type in comment_types:\n",
    "\n",
    "    for filename in os.listdir(f\"./latex/{comment_type}\"):\n",
    "        with open(f\"./latex/{comment_type}/{filename}\", \"r+\") as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r\"\\\\textbackslash Delta\", \"$\\Delta$\", text)\n",
    "            text = re.sub(\"table\", \"table*\", text)\n",
    "            text = re.sub(\"_comments\", \" comments\", text)\n",
    "            f.seek(0)\n",
    "            f.write(text)\n",
    "            f.truncate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_type = \"no_comments\"\n",
    "thresholds = [\"all\", \"50\", \"25\", \"20\", \"15\", \"10\"]\n",
    "for val in thresholds:\n",
    "    df = pd.read_csv(f\"./fold0/{comment_type}/project_stats_{val}.csv\")\n",
    "    # Simplify project names\n",
    "    df[\"project\"] = df.apply(lambda row: re.split(r\"-\\d\", row[\"project\"])[0], axis=1)\n",
    "    table2 = df[[\"project\", \"N\", \"fail_accuracy_improvement\", \"accuracy_improvement\", \"f1_improvement\"]]\n",
    "\n",
    "    if val == \"all\":\n",
    "        df_merge = table2.copy()\n",
    "    else:\n",
    "        df_merge = df_merge.merge(table2, on=\"project\", how=\"left\")\n",
    "        # print(df_merge.columns)\n",
    "        df_merge.rename(\n",
    "            columns={\n",
    "                \"N_x\": f\"N_{last}\",\n",
    "                \"N_y\": f\"N_{val}\",\n",
    "                \"fail_accuracy_improvement_x\": f\"fail_accuracy_improvement_{last}\",\n",
    "                \"fail_accuracy_improvement_y\": f\"fail_accuracy_improvement_{val}\",\n",
    "                \"accuracy_improvement_x\": f\"accuracy_improvement_{last}\",\n",
    "                \"accuracy_improvement_y\": f\"accuracy_improvement_{val}\",\n",
    "                \"f1_improvement_x\": f\"f1_improvement_{last}\",\n",
    "                \"f1_improvement_y\": f\"f1_improvement_{val}\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "    last = val\n",
    "\n",
    "df_merge.to_csv(\"vocab_analysis.csv\")\n",
    "# df_merge[df_merge['project']=='all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_only_df = df_merge[df_merge[\"project\"] != \"all\"]\n",
    "min_sample = 20\n",
    "\n",
    "table_vocab_analysis = pd.DataFrame(\n",
    "    {\n",
    "        \"thresholds\": [\"50%\", \"25%\", \"20%\", \"15%\"],\n",
    "        \"N\": [df_merge.loc[25, \"N_50\"], df_merge.loc[25, \"N_25\"], df_merge.loc[25, \"N_20\"], df_merge.loc[25, \"N_15\"]],\n",
    "        \"fail_accuracy_improvement_total\": [\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_50\"],\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_25\"],\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_20\"],\n",
    "            df_merge.loc[25, \"fail_accuracy_improvement_15\"],\n",
    "        ],\n",
    "        \"accuracy_improvement_total\": [\n",
    "            df_merge.loc[25, \"accuracy_improvement_50\"],\n",
    "            df_merge.loc[25, \"accuracy_improvement_25\"],\n",
    "            df_merge.loc[25, \"accuracy_improvement_20\"],\n",
    "            df_merge.loc[25, \"accuracy_improvement_15\"],\n",
    "        ],\n",
    "        \"f1_improvement_total\": [\n",
    "            df_merge.loc[25, \"f1_improvement_50\"],\n",
    "            df_merge.loc[25, \"f1_improvement_25\"],\n",
    "            df_merge.loc[25, \"f1_improvement_20\"],\n",
    "            df_merge.loc[25, \"f1_improvement_15\"],\n",
    "        ],\n",
    "        \"fail_accuracy_improvement_avg\": [\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_50\"] > min_sample].index, \"fail_accuracy_improvement_50\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_25\"] > min_sample].index, \"fail_accuracy_improvement_25\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_20\"] > min_sample].index, \"fail_accuracy_improvement_20\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_15\"] > min_sample].index, \"fail_accuracy_improvement_15\"].mean(),\n",
    "        ],\n",
    "        \"accuracy_improvement_avg\": [\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_50\"] > min_sample].index, \"accuracy_improvement_50\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_25\"] > min_sample].index, \"accuracy_improvement_25\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_20\"] > min_sample].index, \"accuracy_improvement_20\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_15\"] > min_sample].index, \"accuracy_improvement_15\"].mean(),\n",
    "        ],\n",
    "        \"f1_improvement_avg\": [\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_50\"] > min_sample].index, \"f1_improvement_50\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_25\"] > min_sample].index, \"f1_improvement_25\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_20\"] > min_sample].index, \"f1_improvement_20\"].mean(),\n",
    "            df_merge.loc[project_only_df[project_only_df[\"N_15\"] > min_sample].index, \"f1_improvement_15\"].mean(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "table_vocab_analysis = table_vocab_analysis.astype({\"N\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thresholds</th>\n",
       "      <th>N</th>\n",
       "      <th>fail_accuracy_improvement_total</th>\n",
       "      <th>accuracy_improvement_total</th>\n",
       "      <th>f1_improvement_total</th>\n",
       "      <th>fail_accuracy_improvement_avg</th>\n",
       "      <th>accuracy_improvement_avg</th>\n",
       "      <th>f1_improvement_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50%</td>\n",
       "      <td>145474</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25%</td>\n",
       "      <td>35918</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20%</td>\n",
       "      <td>10156</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15%</td>\n",
       "      <td>2643</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  thresholds       N  fail_accuracy_improvement_total  \\\n",
       "0        50%  145474                           -0.000   \n",
       "1        25%   35918                           -0.016   \n",
       "2        20%   10156                           -0.050   \n",
       "3        15%    2643                           -0.022   \n",
       "\n",
       "   accuracy_improvement_total  f1_improvement_total  \\\n",
       "0                      -0.010                -0.005   \n",
       "1                       0.011                 0.006   \n",
       "2                      -0.007                -0.002   \n",
       "3                      -0.074                -0.045   \n",
       "\n",
       "   fail_accuracy_improvement_avg  accuracy_improvement_avg  f1_improvement_avg  \n",
       "0                         -0.000                    -0.005              -0.003  \n",
       "1                         -0.006                    -0.026              -0.021  \n",
       "2                         -0.064                    -0.001               0.024  \n",
       "3                          0.014                    -0.070              -0.054  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in [\"fail_accuracy_improvement_total\", \"accuracy_improvement_total\", \"f1_improvement_total\"]:\n",
    "    string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    table_vocab_analysis[col] = table_vocab_analysis[col].apply(lambda x: np.round(x - df_merge.loc[25, f\"{string}_all\"], 3))\n",
    "\n",
    "for col in [\"fail_accuracy_improvement_avg\", \"accuracy_improvement_avg\", \"f1_improvement_avg\"]:\n",
    "    string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    table_vocab_analysis[col] = table_vocab_analysis[col].apply(lambda x: np.round(x - project_only_df[f\"{string}_all\"].mean(), 3))\n",
    "\n",
    "table_vocab_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_vocab_analysis.to_latex(\n",
    "    f\"./latex/vocab_analysis.tex\",\n",
    "    index=False,\n",
    "    caption=f\"Performance of SEER on New Data with varying minimum \\% of tokens in-vocab threshold.\",\n",
    "    label=f\"tab:vocab_analysis\",\n",
    ")\n",
    "\n",
    "with open(f\"./latex/vocab_analysis.tex\", \"r+\") as f:\n",
    "    text = f.read()\n",
    "    text = re.sub(\"table\", \"table*\", text)\n",
    "    text = re.sub(\"accuracy\", \"acc.\", text)\n",
    "    text = re.sub(\"_improvement\", \"_$\\Delta$\", text)\n",
    "    text = re.sub(\"_total\", \"_all\", text)\n",
    "    text = re.sub(\"_avg\", \"_project\\_avg\", text)\n",
    "    f.seek(0)\n",
    "    f.write(text)\n",
    "    f.truncate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robustness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment_type in comment_types:\n",
    "    df = pd.read_csv(f\"./fold0/{comment_type}/project_stats_all.csv\")\n",
    "    # Simplify project names\n",
    "    df[\"project\"] = df.apply(lambda row: re.split(r\"-\\d\", row[\"project\"])[0], axis=1)\n",
    "    table2 = df[[\"project\", \"N\", \"fail_accuracy_improvement\", \"accuracy_improvement\", \"f1_improvement\"]].copy()\n",
    "    table2.rename(\n",
    "        columns={\n",
    "            \"N\": f\"N_{comment_type}\",\n",
    "            \"fail_accuracy_improvement\": f\"fail_accuracy_improvement_{comment_type}\",\n",
    "            \"accuracy_improvement\": f\"accuracy_improvement_{comment_type}\",\n",
    "            \"f1_improvement\": f\"f1_improvement_{comment_type}\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    if comment_type == comment_types[0]:\n",
    "        df_merge_comments = table2.copy()\n",
    "    else:\n",
    "        df_merge_comments = df_merge_comments.merge(table2, on=\"project\", how=\"left\")\n",
    "        # print(df_merge_comments.columns)\n",
    "    # last = comment_type\n",
    "\n",
    "# df_merge_comments.rename(\n",
    "#     columns={\n",
    "#         \"N_x\": f\"N_{comment_types[0]}\",\n",
    "#         \"N_y\": f\"N_{comment_types[1]}\",\n",
    "#         \"N\": f\"N_{comment_types[2]}\",\n",
    "#         \"fail_accuracy_improvement_x\": f\"fail_accuracy_improvement_{comment_types[0]}\",\n",
    "#         \"fail_accuracy_improvement_y\": f\"fail_accuracy_improvement_{comment_types[1]}\",\n",
    "#         \"fail_accuracy_improvement\": f\"fail_accuracy_improvement_{comment_types[2]}\",\n",
    "#         \"accuracy_improvement_x\": f\"accuracy_improvement_{comment_types[0]}\",\n",
    "#         \"accuracy_improvement_y\": f\"accuracy_improvement_{comment_types[1]}\",\n",
    "#         \"accuracy_improvement\": f\"accuracy_improvement_{comment_types[2]}\",\n",
    "#         \"f1_improvement_x\": f\"f1_improvement_{comment_types[0]}\",\n",
    "#         \"f1_improvement_y\": f\"f1_improvement_{comment_types[1]}\",\n",
    "#         \"f1_improvement\": f\"f1_improvement_{comment_types[2]}\",\n",
    "#     },\n",
    "#     inplace=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_only_comments_df = df_merge_comments[df_merge_comments[\"project\"] != \"all\"]\n",
    "min_sample = 20\n",
    "\n",
    "table_comment_analysis = pd.DataFrame(\n",
    "    {\n",
    "        \"comment_types\": comment_types,\n",
    "        \"N\": [df_merge_comments.loc[25, f\"N_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"fail_accuracy_improvement_total\": [df_merge_comments.loc[25, f\"fail_accuracy_improvement_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"accuracy_improvement_total\": [df_merge_comments.loc[25, f\"accuracy_improvement_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"f1_improvement_total\": [df_merge_comments.loc[25, f\"f1_improvement_{comment_type}\"] for comment_type in comment_types],\n",
    "        \"fail_accuracy_improvement_avg\": [\n",
    "            df_merge_comments.loc[project_only_comments_df[project_only_comments_df[f\"N_{comment_type}\"] > min_sample].index, f\"fail_accuracy_improvement_{comment_type}\"].mean()\n",
    "            for comment_type in comment_types\n",
    "        ],\n",
    "        \"accuracy_improvement_avg\": [\n",
    "            df_merge_comments.loc[project_only_comments_df[project_only_comments_df[f\"N_{comment_type}\"] > min_sample].index, f\"accuracy_improvement_{comment_type}\"].mean()\n",
    "            for comment_type in comment_types\n",
    "        ],\n",
    "        \"f1_improvement_avg\": [\n",
    "            df_merge_comments.loc[project_only_comments_df[project_only_comments_df[f\"N_{comment_type}\"] > min_sample].index, f\"f1_improvement_{comment_type}\"].mean()\n",
    "            for comment_type in comment_types\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "table_comment_analysis = table_comment_analysis.astype({\"N\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_types</th>\n",
       "      <th>fail_accuracy_improvement_total</th>\n",
       "      <th>accuracy_improvement_total</th>\n",
       "      <th>f1_improvement_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_comments</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comments</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>added_test_comments</td>\n",
       "      <td>-0.0086</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>added_code_comments</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>added_CT_comments</td>\n",
       "      <td>-0.0558</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         comment_types  fail_accuracy_improvement_total  \\\n",
       "0          no_comments                           0.0000   \n",
       "1             comments                           0.0055   \n",
       "2  added_test_comments                          -0.0086   \n",
       "3  added_code_comments                          -0.0446   \n",
       "4    added_CT_comments                          -0.0558   \n",
       "\n",
       "   accuracy_improvement_total  f1_improvement_total  \n",
       "0                      0.0000                0.0000  \n",
       "1                     -0.0002               -0.0002  \n",
       "2                      0.0005                0.0003  \n",
       "3                      0.0096                0.0055  \n",
       "4                      0.0135                0.0077  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in [\"fail_accuracy_improvement_total\", \"accuracy_improvement_total\", \"f1_improvement_total\"]:\n",
    "    string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    table_comment_analysis[col] = table_comment_analysis[col].apply(lambda x: np.round(x - df_merge_comments.loc[25, f\"{string}_no_comments\"], 4))\n",
    "\n",
    "for col in [\"fail_accuracy_improvement_avg\", \"accuracy_improvement_avg\", \"f1_improvement_avg\"]:\n",
    "    table_comment_analysis.drop(columns=[col], inplace=True)\n",
    "    # string = \"_\".join(col.split(\"_\")[:-1])\n",
    "    # table_comment_analysis[col] = table_comment_analysis[col].apply(lambda x: np.round(x - project_only_comments_df[f\"{string}_no_comments\"].mean(), 4))\n",
    "\n",
    "table_comment_analysis.drop(columns=[\"N\"], inplace=True)\n",
    "table_comment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_comment_analysis.to_latex(\n",
    "    f\"./latex/comment_analysis.tex\",\n",
    "    index=False,\n",
    "    caption=f\"Performance of SEER on New Data with different comment types (compared to a no-comment baseline).\",\n",
    "    label=f\"tab:comment_analysis\",\n",
    ")\n",
    "\n",
    "with open(f\"./latex/comment_analysis.tex\", \"r+\") as f:\n",
    "    text = f.read()\n",
    "    # text = re.sub(\"table\", \"table*\", text)\n",
    "    text = re.sub(\"accuracy\", \"acc.\", text)\n",
    "    text = re.sub(\"\\\\\\_improvement\", \"\", text)\n",
    "    text = re.sub(\"_total\", \"_all\", text)\n",
    "    text = re.sub(\"_avg\", \"_project\\_avg\", text)\n",
    "    f.seek(0)\n",
    "    f.write(text)\n",
    "    f.truncate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_unique = pd.read_csv(f\"./similarity_analysis/similarity_unique_mut.csv\")\n",
    "# Simplify project names\n",
    "df_common_unique[\"triplets\"] = df_common_unique.apply(lambda row: re.split(r\"-\\d\", row[\"triplets\"])[0], axis=1)\n",
    "\n",
    "df_common_unique.rename(\n",
    "    columns={\n",
    "        \"phase2\": \"SEER\",\n",
    "        \"triplets\": \"New Data\",\n",
    "        \"triplets_unique_count\": \"New Data Count\",\n",
    "        \"phase2_unique_count\": \"SEER Count\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df_common_unique[[\"SEER\", \"New Data\", \"SEER Count\", \"New Data Count\"]].to_latex(\n",
    "    f\"./latex/common_projects_unique.tex\",\n",
    "    index=False,\n",
    "    caption=f\"Unique Methods Under Test\",\n",
    "    label=f\"tab:common_unique_MUT\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of robustness fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_no_comment</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>predicted_added_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132719</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132720</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132721</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132722</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132723</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        predicted_no_comment  Actual Label  predicted_added_code\n",
       "132719                     0             1                     1\n",
       "132720                     0             1                     1\n",
       "132721                     0             1                     1\n",
       "132722                     1             0                     1\n",
       "132723                     1             1                     1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_comment_results = pd.read_csv(\"fold0/no_comments/jsoup/test_stats.csv\").rename(columns={\"Predicted Label\": \"predicted_no_comment\"})\n",
    "df_added_code_results = pd.read_csv(\"fold0/added_code_comments/jsoup/test_stats.csv\").rename(columns={\"Predicted Label\": \"predicted_added_code\"})\n",
    "df_all = df_no_comment_results.copy()\n",
    "df_all[\"predicted_added_code\"] = df_added_code_results[\"predicted_added_code\"]\n",
    "df_all.index = df_all.index + 132719\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclasify_pass = df_all[(df_all[\"predicted_no_comment\"] == df_all[\"Actual Label\"]) & (df_all[\"predicted_added_code\"] != df_all[\"Actual Label\"]) & (df_all[\"Actual Label\"] == 1)]\n",
    "misclasify_fail = df_all[(df_all[\"predicted_no_comment\"] == df_all[\"Actual Label\"]) & (df_all[\"predicted_added_code\"] != df_all[\"Actual Label\"]) & (df_all[\"Actual Label\"] == 0)]\n",
    "# df_all['Actual Label'] == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
